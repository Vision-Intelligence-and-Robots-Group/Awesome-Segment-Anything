# Awesome-Segment-Anything

Tribute to Meta AI's Segment Anything Model (SAM)

A collection of projects, papers, and source code for SAM and related studies.

**Keywords:** Segment Anything Model, Segment Anything, SAM, awesome

> **CATALOGUE**
>
>[Origin of the Study](#quick-start) :heartpulse: [Project & Toolbox](#tool) :heartpulse: [Lecture & Notes](#workshop) :heartpulse: [Papers](#papers-by-categories)

## 1 Origin of the Study <span id='quick-start'></span>

**Fundemental Models**

+ **[SAM]** Segment Anything (2023)[[paper]](https://arxiv.org/pdf/2304.02643.pdf) [[Project]](https://github.com/facebookresearch/segment-anything)![GitHub stars](https://img.shields.io/github/stars/facebookresearch/segment-anything.svg?logo=github&label=Stars)

+ **[SEEM]** Segment Everything Everywhere All at Once (2023)[[paper]](https://arxiv.org/pdf/2304.06718.pdf)[[Project]](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)![GitHub stars](https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once.svg?logo=github&label=Stars)

## 2 Project & Toolbox<span id='tool'>
  

+ **[Awesome Segment-Anything Extensions]** Awesome Segment-Anything Extensions [paper][[code]](https://github.com/JerryX1110/awesome-segment-anything-extensions)![GitHub stars](https://img.shields.io/github/stars/JerryX1110/awesome-segment-anything-extensions.svg?logo=github&label=Stars)

+ **[Awesome Anything]** A curated list of general AI methods for Anything [paper][[code]](https://github.com/VainF/Awesome-Anything)![GitHub stars](https://img.shields.io/github/stars/VainF/Awesome-Anything.svg?logo=github&label=Stars)

|Preview|Project|
|------|------|
|![preview](https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/assets/acoustics/gsam_whisper_inpainting_demo.png)|**[Grounded-Segment-Anything]** Grounded-Segment-Anything(DINO + SAM) [[paper]](https://arxiv.org/abs/2303.05499)[[code]](https://github.com/IDEA-Research/Grounded-Segment-Anything)![GitHub stars](https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything.svg?logo=github&label=Stars)|
|![preview](https://github.com/Cheems-Seminar/grounded-segment-any-parts/raw/main/assets/dog2zebra.jpg)|**[Grounded Segment Anything: From Objects to Parts]** Support text prompt input(GLIP/VLPart + SAM)[paper][[code]](https://github.com/Cheems-Seminar/grounded-segment-any-parts)![GitHub Repo stars](https://img.shields.io/github/stars/Cheems-Seminar/grounded-segment-any-parts?logo=github&style=flat-square)|
|![preview](https://github.com/fudan-zvg/Semantic-Segment-Anything/raw/main/figures/SSA_motivation.png)|**[Semantic-Segment-Anything]** A pipeline on top of SAM to predict semantic category for each mask [paper][[code]](https://github.com/fudan-zvg/Semantic-Segment-Anything)![GitHub stars](https://img.shields.io/github/stars/fudan-zvg/Semantic-Segment-Anything.svg?logo=github&label=Stars)|
|![preview](https://github.com/caoyunkang/GroundedSAM-zero-shot-anomaly-detection/raw/master/assets/demo_results.png)|**[GroundedSAM-zero-shot-anomaly-detection]** Segment any anomaly[papaer][[code]](https://github.com/caoyunkang/GroundedSAM-zero-shot-anomaly-detection)![GitHub Repo stars](https://img.shields.io/github/stars/caoyunkang/GroundedSAM-zero-shot-anomaly-detection?logo=github&style=flat-square)|
|![preview](https://github.com/sail-sg/EditAnything/raw/main/images/sample_cat_eye.jpg)|**[EditAnything]** EditAnything [paper][[code]](https://github.com/sail-sg/EditAnything)![GitHub stars](https://img.shields.io/github/stars/sail-sg/EditAnything.svg?logo=github&label=Stars)|
|![preview](https://user-images.githubusercontent.com/14961526/230437084-79ef6e02-a254-421e-bd4c-32e87415c623.png)|**[sd-webui-segment-anything]** extension for helping stable diffusion webui with inpainting [paper][[code]](https://github.com/continue-revolution/sd-webui-segment-anything)![GitHub stars](https://img.shields.io/github/stars/continue-revolution/sd-webui-segment-anything.svg?logo=github&label=Stars)|
|![preview](https://github.com/anuragxel/salt/raw/main/assets/how-it-works.gif)|**[SALT]** Segment Anything Labelling Tool [paper][[code]](https://github.com/anuragxel/salt)![GitHub stars](https://img.shields.io/github/stars/anuragxel/salt.svg?logo=github&label=Stars)|
|![preview](https://github.com/PengtaoJiang/SAM-CLIP/raw/main/imgs/pipeline.png)|**[SAM-CLIP]** Segment Anything CLIP [paper][[code]](https://github.com/PengtaoJiang/Segment-Anything-CLIP)![GitHub stars](https://img.shields.io/github/stars/PengtaoJiang/Segment-Anything-CLIP.svg?logo=github&label=Stars)|
|![preview](https://github.com/RockeyCoss/Prompt-Segment-Anything/raw/master/assets/example1.jpg)|**[Prompt-Segment-Anything]** An implementation of SAM[parper][[code]](https://github.com/RockeyCoss/Prompt-Segment-Anything)![GitHub Repo stars](https://img.shields.io/github/stars/RockeyCoss/Prompt-Segment-Anything?logo=github&style=flat-square)|
|![preview](https://github.com/Vision-Intelligence-and-Robots-Group/count-anything/raw/main/example.png)|**[Count-Anything]** Few-shot SAM Counting[parper][[code]](https://github.com/Vision-Intelligence-and-Robots-Group/count-anything)![GitHub Repo stars](https://img.shields.io/github/stars/Vision-Intelligence-and-Robots-Group/count-anything?logo=github&style=flat-square)|
|![preview](https://baai-seggpt.hf.space/file/rainbow2.gif)|**[SegGPT: Vision Foundation Models]** One touch for segmentation in all images (SAM+SegGPT)[[paper]](https://arxiv.org/abs/2304.03284)[[code]](https://github.com/baaivision/Painter)![GitHub Repo stars]![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/Painter?logo=github&style=flat-square)|
|![preview](https://github.com/QianXuna/Myimg/blob/main/image/ay0uq-jd22c.gif?raw=true)|**[napari-segment-anything]** Image viewer plugin of SAM[paper][[code]](https://github.com/JoOkuma/napari-segment-anything)![GitHub Repo stars]![GitHub Repo stars](https://img.shields.io/github/stars/JoOkuma/napari-segment-anything?logo=github&style=flat-square)|

## 3 Lecture & Notes<span id='workshop'>

**How to | roboflow** how-to-segment-anything-with-sam [[blog]](https://github.com/roboflow/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb)


## 4 Papers <span id='papers-by-categories'></span>
  
### Image Segmentation & Medical Image Segmentation

+ **[Zero-shot Segmentation]** Segment Anything Model (SAM) for Digital Pathology: Assess Zero-shot Segmentation on Whole Slide Imaging[[paper]](https://arxiv.org/abs/2304.04155)[[code]](https://github.com/BingfengYan/VISAM)![GitHub stars](https://img.shields.io/github/stars/BingfengYan/VISAM.svg?logo=github&label=Stars)

+ **[generic segmentation]** Segment Anything Is Not Always Perfect: An Investigation of SAM on Different Real-world Applications [[paper]](http://arxiv.org/abs/2304.05750v2)[code]
+ **[Medical Image segmentation]** SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM [[paper]](http://arxiv.org/abs/2304.05622v1)[[code]](https://github.com/bingogome/samm)

+ **[Medical Image segmentation]** SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model [[paper]]http://arxiv.org/abs/2304.05396v1 [code]

+ **[Camouflaged Object Segmentation]** SAM Struggles in Concealed Scenes -- Empirical Study on "Segment Anything"[[paper]](https://arxiv.org/abs/2304.06022)[code]

+ **[Brain Extraction]** Brain Extraction comparing Segment Anything Model (SAM) and FSL Brain Extraction Tool[[paper]](https://arxiv.org/abs/2304.04738)[code]

### Object Detection & Tracking

+ **[Camouflaged Object Detection]** Can SAM Segment Anything? When SAM Meets Camouflaged Object Detection[[paper]](https://arxiv.org/abs/2304.04709)[[code]](https://github.com/luckybird1994/SAMCOD)![GitHub stars](https://img.shields.io/github/stars/luckybird1994/SAMCOD.svg?logo=github&label=Stars)

+ **[Multi-Object Tracking]** CO-MOT[paper][[code]](https://github.com/BingfengYan/VISAM)![GitHub stars](https://img.shields.io/github/stars/BingfengYan/VISAM.svg?logo=github&label=Stars)

### Explaning and Supporting Basic Models

+ **[CLIP]** CLIP Surgery for Better Explainability with Enhancement in Open-Vocabulary Tasks [[paper]](http://arxiv.org/abs/2304.05653v1)[[code]](https://github.com/xmed-lab/clip_surgery)

### Arxiv-daily-update
[Click here](https://github.com/Vision-Intelligence-and-Robots-Group/Awesome-Segment-Anything-Model/blob/main/arxiv-daily-docs/README.md) to check the daily-updated paper list!

